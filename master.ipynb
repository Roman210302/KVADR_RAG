{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7ed4f8b-a87b-47ea-8fb4-d2cf094e558c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T22:44:48.444460Z",
     "iopub.status.busy": "2024-09-13T22:44:48.443288Z",
     "iopub.status.idle": "2024-09-13T22:45:17.676726Z",
     "shell.execute_reply": "2024-09-13T22:45:17.675899Z",
     "shell.execute_reply.started": "2024-09-13T22:44:48.444415Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-09-13 22:45:00.980360: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-13 22:45:02.841911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-13 22:45:06.700471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_milvus import Milvus\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from huggingface_hub import hf_hub_download\n",
    "from vllm import LLM, SamplingParams\n",
    "from langchain_community.llms import VLLM\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_utilization, context_recall, answer_correctness \n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.documents.compressor import BaseDocumentCompressor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da6f3a01-2fb3-486b-967b-b6de5e1cb7f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T22:45:28.759093Z",
     "iopub.status.busy": "2024-09-13T22:45:28.757878Z",
     "iopub.status.idle": "2024-09-13T22:45:28.787183Z",
     "shell.execute_reply": "2024-09-13T22:45:28.786458Z",
     "shell.execute_reply.started": "2024-09-13T22:45:28.759038Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = { # The first value in every position is baseline\n",
    "    'model_name': 'llama3.1-8b-q4',  # llama3.1-8b-q4 / gemma-2-9b-it-simpo-q4 / tlite-q4\n",
    "    'embed_model_name_short': 'e5l', # e5l (multilingual-e5-large) / bgem3 (bge-m3)\n",
    "    'chunk_size': 512, # 512/128, 1024/256, 256/64 \n",
    "    'chunk_overlap': 128,\n",
    "    'llm_framework': 'VLLM', # VLLM, LLamaCpp, Ollama (Ollama is only for gemma2 model)\n",
    "    'vectorstore_name': 'MILVUS', # Vector Database MILVUS / FAISS\n",
    "    'retriever_name': 'vectorstore', # 'vectorstore' / 'ensemble' (BM25 + vertorstore)\n",
    "    'retriever_k': 4, # 4, if reranker, then retriever_k=30 (reranker returns 4)\n",
    "    'compressor_name': None, # None / 'cross_encoder_reranker' / 'gluing_chunks' \n",
    "    'chain_type': 'stuff', # 'stuff'\n",
    "}\n",
    "# If chosen a HybridSearch\n",
    "ensemble_config = {\n",
    "    'ensemble_retrievers_names': ['BM25', 'vectorstore'], \n",
    "    'ensemble_retrievers_weights': [0.4, 0.6], \n",
    "}\n",
    "\n",
    "llama_config = {\n",
    "    'repo_id': 'lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "    'filename': 'Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf',\n",
    "    'tokenizer': 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4'\n",
    "    }\n",
    "\n",
    "gemma_config = {\n",
    "    'repo_id': \"mannix/gemma2-9b-simpo\",\n",
    "    'llm_framework': 'Ollama'\n",
    "    }\n",
    "\n",
    "tlite_config = {\n",
    "    'repo_id': 'mradermacher/saiga_tlite_8b-GGUF',\n",
    "    'filename': 'saiga_tlite_8b.Q4_K_M.gguf',\n",
    "    'tokenizer': 'IlyaGusev/saiga_tlite_8b'\n",
    "    }\n",
    "\n",
    "reranker_config = {\n",
    "    'reranker_model': \"BAAI/bge-reranker-v2-m3\",\n",
    "    'retriever_k': 30\n",
    "}\n",
    "\n",
    "\n",
    "def update_config_with_model(config, llama_config, gemma_config, tlite_config):\n",
    "    \"\"\"\n",
    "    Update the configuration based on the selected model.\n",
    "\n",
    "    Parameters:\n",
    "    config (dict): The configuration to update.\n",
    "    llama_config (dict): The configuration for the Llama model.\n",
    "    gemma_config (dict): The configuration for the Gemma model.\n",
    "    tlite_config (dict): The configuration for the T-lite model.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If an incorrect model_name is selected.\n",
    "    \"\"\"\n",
    "    if config['model_name'] == 'llama3.1-8b-q4':\n",
    "        config.update(llama_config)\n",
    "    elif config['model_name'] == 'gemma-2-9b-it-simpo-q4':\n",
    "        config.update(gemma_config)\n",
    "    elif config['model_name'] == 'tlite-q4':\n",
    "        config.update(tlite_config)\n",
    "    else:\n",
    "        ValueError('Incorrect model_name: choose from llama3.1-8b-q4, gemma-2-9b-it-simpo-q4, or tlite-q4')\n",
    "    \n",
    "    if config['embed_model_name_short'] == 'e5l':\n",
    "        config['embedding_model'] = \"intfloat/multilingual-e5-large\"\n",
    "    elif config['embed_model_name_short'] == 'bgem3':\n",
    "        config['embedding_model'] = 'BAAI/bge-m3'\n",
    "    \n",
    "    if config['retriever_name'] == 'ensemble':\n",
    "        config.update(ensemble_config)\n",
    "    \n",
    "    if config['compressor_name'] == 'cross_encoder_reranker':\n",
    "        config.update(reranker_config)\n",
    "\n",
    "\n",
    "update_config_with_model(config, llama_config, gemma_config, tlite_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "097d2e02-e7ef-4be2-9f81-c37db053eebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T22:45:30.356380Z",
     "iopub.status.busy": "2024-09-13T22:45:30.355374Z",
     "iopub.status.idle": "2024-09-13T22:45:30.482384Z",
     "shell.execute_reply": "2024-09-13T22:45:30.481517Z",
     "shell.execute_reply.started": "2024-09-13T22:45:30.356345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# внутри используется реранкер и эвристики\n",
    "class ChunkCompressor(BaseDocumentCompressor):\n",
    "    \"\"\"\n",
    "    A class for compressing documents into chunks.\n",
    "    This class is a subclass of BaseDocumentCompressor and overrides the compress_documents method.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks: list\n",
    "    \"\"\"\n",
    "    A list of chunks.\n",
    "    Each chunk is a Document object.\n",
    "    \"\"\"\n",
    "\n",
    "    chunk_overlap: int\n",
    "    \"\"\"\n",
    "    The amount of overlap between chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    def compress_documents(self, inputs, query=None, callbacks=None):\n",
    "        \"\"\"\n",
    "        Compress the given chunks into extended chunks(add a neighboor chunks to answer).\n",
    "\n",
    "        Parameters:\n",
    "        inputs (list): The list of chunks to compress.\n",
    "        query (str): The query to use for compression.\n",
    "        callbacks (list): The list of callbacks to use for compression.\n",
    "\n",
    "        Returns:\n",
    "        list: The list of compressed chunks.\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for chunk in inputs:\n",
    "            current_id = chunk.metadata['chunk_index']\n",
    "            new_chunk = Document(page_content=chunk.page_content, metadata=chunk.metadata)\n",
    "            \n",
    "            # Add a left-context chunk\n",
    "            if current_id > 0:\n",
    "                left_neighbor = self.chunks[current_id - 1]\n",
    "                new_chunk.page_content = left_neighbor.page_content[:-self.config['chunk_overlap']] + new_chunk.page_content\n",
    "            \n",
    "            # Add a right-context chunk\n",
    "            if current_id < len(self.chunks) - 1:\n",
    "                right_neighbor = self.chunks[current_id + 1]\n",
    "                new_chunk.page_content += right_neighbor.page_content[self.config['chunk_overlap']:]\n",
    "            # \n",
    "            outputs.append(new_chunk)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e40e91bb-01e3-4168-96b1-bbb0e7137bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T22:45:32.394009Z",
     "iopub.status.busy": "2024-09-13T22:45:32.392866Z",
     "iopub.status.idle": "2024-09-13T22:49:48.763407Z",
     "shell.execute_reply": "2024-09-13T22:49:48.762563Z",
     "shell.execute_reply.started": "2024-09-13T22:45:32.393972Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-13 22:47:16 config.py:1647] Downcasting torch.float32 to torch.float16.\n",
      "WARNING 09-13 22:47:16 config.py:330] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 09-13 22:47:16 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='/tmp/xdg_cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', speculative_config=None, tokenizer='/tmp/xdg_cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.GGUF, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/tmp/xdg_cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-13 22:48:11 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-13 22:48:11 selector.py:116] Using XFormers backend.\n",
      "INFO 09-13 22:48:11 model_runner.py:915] Starting to load model /tmp/xdg_cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf...\n",
      "INFO 09-13 22:48:26 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-13 22:48:26 selector.py:116] Using XFormers backend.\n",
      "INFO 09-13 22:48:43 model_runner.py:926] Loading model weights took 4.7372 GB\n",
      "INFO 09-13 22:48:59 gpu_executor.py:122] # GPU blocks: 8388, # CPU blocks: 2048\n",
      "INFO 09-13 22:49:01 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-13 22:49:01 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-13 22:49:48 model_runner.py:1335] Graph capturing finished in 47 secs.\n"
     ]
    }
   ],
   "source": [
    "class CustomRAGPipeline:\n",
    "    \"\"\"\n",
    "    A class for a custom RAG (Retrieval Augmented Generation) pipeline.\n",
    "    This pipeline is designed to process and retrieve information from a given set of documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 documents_path: str,\n",
    "                 config: dict,\n",
    "                 recalc_embedding: bool = False,\n",
    "                 recalc_chunks: bool = False,\n",
    "                 ):\n",
    "        \"\"\"        \n",
    "        Initialize the class object with the given documents path and configuration.\n",
    "\n",
    "        Parameters:\n",
    "        documents_path (str): The path to the text documents.\n",
    "        config (dict): The configuration for the pipeline.\n",
    "        recalc_embedding (bool): Whether to recalculate the embeddings.\n",
    "        recalc_chunks (bool): Whether to recalculate the chunks.\n",
    "        \"\"\"        \n",
    "        self.config = config\n",
    "        self.documents_path = documents_path\n",
    "        self.embedding_model = self.config['embedding_model']\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
    "        self.chunks = None\n",
    "        \n",
    "        self.retriever = None\n",
    "        self.compressor = None\n",
    "\n",
    "        self.vectorstore_path = '_'.join([self.config['embed_model_name_short'], \n",
    "                                          self.config['vectorstore_name'], \n",
    "                                          str(self.config['chunk_size']), \n",
    "                                          str(self.config['chunk_overlap'])]\n",
    "                                        )\n",
    "        \n",
    "        self.chunks_path = '_'.join(['chunks', \n",
    "                                     str(self.config['chunk_size']), \n",
    "                                     str(self.config['chunk_overlap'])]\n",
    "                                   ) + '.pkl'\n",
    "\n",
    "        if not recalc_embedding:\n",
    "            if os.path.exists(self.vectorstore_path) and self.config['vectorstore_name'] == 'FAISS':\n",
    "                self.vectorstore = FAISS.load_local(self.vectorstore_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "            elif os.path.isfile(f\"{self.vectorstore_path}.db\") and self.config['vectorstore_name'] == 'MILVUS':\n",
    "                self.vectorstore = Milvus(\n",
    "                    self.embeddings,\n",
    "                    connection_args={\"uri\": f\"./{self.vectorstore_path}.db\"},\n",
    "                    collection_name=\"RAG\",\n",
    "                )\n",
    "        \n",
    "        if not recalc_chunks:\n",
    "            if os.path.exists(self.chunks_path):\n",
    "                with open(self.chunks_path, \"rb\") as f:\n",
    "                    self.chunks = pickle.load(f)\n",
    "\n",
    "    # Load chosen model\n",
    "        if self.config['llm_framework'] == 'VLLM':\n",
    "            self.llm = self.load_vllm_model()\n",
    "        elif self.config['llm_framework'] == 'LLamaCpp':\n",
    "            self.llm = self.load_llama_cpp_model()\n",
    "        elif self.config['llm_framework'] == 'Ollama':\n",
    "            self.llm = self.load_ollama_model()\n",
    "            \n",
    "            \n",
    "    def load_vllm_model(self):\n",
    "        \"\"\"\n",
    "        Load the vLLM model from HuggingFace Hub.\n",
    "\n",
    "        Returns:\n",
    "        vllm_llm (VLLM): The loaded vLLM model.\n",
    "        \"\"\"\n",
    "        repo_id = self.config['repo_id']\n",
    "        filename = self.config['filename']\n",
    "        tokenizer = self.config['tokenizer']\n",
    "        model_path = hf_hub_download(repo_id, filename=filename)\n",
    "        \n",
    "        # Initialize vLLM with the downloaded model\n",
    "        vllm_llm = VLLM(model=model_path,\n",
    "                        vllm_kwargs={\"quantization\": \"awq\", \n",
    "                                     'max_model_len': 8192,\n",
    "                                     'gpu_memory_utilization': 0.75},\n",
    "                        temperature=0.75,\n",
    "                        stop=[\"<|eot_id|>\"]\n",
    "                        )\n",
    "        \n",
    "        return vllm_llm\n",
    "\n",
    "\n",
    "    def load_llama_cpp_model(self):\n",
    "        \"\"\"\n",
    "        Initialize the LlamaCpp model.\n",
    "\n",
    "        Returns:\n",
    "        llama_cpp_llm (LlamaCpp): The initialized LlamaCpp model.\n",
    "        \"\"\"\n",
    "        repo_id = self.config['repo_id']\n",
    "        filename = self.config['filename']\n",
    "        model_path = hf_hub_download(repo_id, filename=filename)\n",
    "        \n",
    "        # Initialize LlamaCpp\n",
    "        llama_cpp_llm = LlamaCpp(model_path=model_path,\n",
    "                                temperature=0.8,\n",
    "                                top_p=0.95,\n",
    "                                top_k=30,\n",
    "                                max_tokens=64,\n",
    "                                n_ctx=8192,\n",
    "                                n_parts=-1,\n",
    "                                n_gpu_layers=64,\n",
    "                                n_threads=8,\n",
    "                                frequency_penalty=1.1,\n",
    "                                verbose=True,\n",
    "                                stop=[\"<|eot_id|>\"],  # Stopping on the EOS token\n",
    "                                )\n",
    "        \n",
    "        return llama_cpp_llm\n",
    "\n",
    "    \n",
    "    def load_ollama_model(self):\n",
    "        \"\"\"\n",
    "        Pull the Ollama model from the repository.\n",
    "\n",
    "        Returns:\n",
    "        OllamaLLM: The Ollama model.\n",
    "        \"\"\"\n",
    "        model_name = self.config['repo_id']\n",
    "        \n",
    "        command = f\"ollama pull {model_name}\"\n",
    "        # Try to pull Ollama model\n",
    "        try:\n",
    "            subprocess.run(command, shell=True, check=True)\n",
    "            print(f'Pullled the model {model_name} successfully')\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model {model_name}: {e}\")\n",
    "        \n",
    "        return OllamaLLM(\n",
    "            model=model_name,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            top_k=30,\n",
    "            max_tokens=1024,\n",
    "            stop=[\"<|eot_id|>\"]\n",
    "        )\n",
    "    \n",
    "    def load_and_process_documents(self):\n",
    "        \"\"\"\n",
    "        Load and process the documents.\n",
    "        If the retriever is an \"ensemble\", or the retriever is an \"BM25\", \n",
    "        or the compressor is a \"gluing_chunks\", or the vectorstore is not initialized,\n",
    "        the documents are split into chunks and added to the vectorstore.\n",
    "        \"\"\"\n",
    "        if (self.config['retriever_name'] == 'ensemble') \\\n",
    "            or (self.config['retriever_name'] == 'BM25') \\\n",
    "            or (self.config['compressor_name'] == 'gluing_chunks') \\\n",
    "            or (not self.vectorstore):\n",
    "\n",
    "            loader = TextLoader(self.documents_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Split the documents into chunks\n",
    "            text_splitter = CharacterTextSplitter(\n",
    "                        separator=\" \",\n",
    "                        chunk_size=self.config['chunk_size'],\n",
    "                        chunk_overlap=self.config['chunk_overlap'],\n",
    "                        length_function=len,\n",
    "                        is_separator_regex=False,\n",
    "                    )\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "\n",
    "            # Add chuck index to metadata to find neighborous-chunks after retrieving\n",
    "            for index, document in enumerate(texts):\n",
    "                document.metadata[\"chunk_index\"] = index\n",
    "            self.chunks = texts\n",
    "            \n",
    "            with open(self.chunks_path, \"wb\") as f:\n",
    "                pickle.dump(self.chunks, f)\n",
    "            \n",
    "            if not self.vectorstore:\n",
    "                if self.config['vectorstore_name'] == 'FAISS':\n",
    "                    # Create a FAISS vector store from the documents\n",
    "                    self.vectorstore = FAISS.from_documents(texts, self.embeddings)\n",
    "                    self.vectorstore.save_local(self.vectorstore_path)\n",
    "                elif self.config['vectorstore_name'] == 'MILVUS':\n",
    "                    # Create a MILVUS vector store from the documents\n",
    "                    self.vectorstore = Milvus.from_documents(\n",
    "                        texts,\n",
    "                        self.embeddings,\n",
    "                        collection_name=\"RAG\",\n",
    "                        connection_args={\"uri\": f\"./{self.vectorstore_path}.db\"})\n",
    "    \n",
    "    \n",
    "    def init_retriever(self):   \n",
    "        \"\"\"\n",
    "        Initialize the retriever.\n",
    "        If the retriever name is 'ensemble', 'BM25', or 'vectorstore', the corresponding retriever is initialized.\n",
    "\n",
    "        Returns:\n",
    "        retriever: The initialized retriever.\n",
    "        \"\"\"\n",
    "        if self.config['retriever_name'] == 'ensemble':\n",
    "            # Build an ensemble retriever based on the config data\n",
    "            retrievers = []\n",
    "            for retriever in self.config['ensemble_retrievers_names']:\n",
    "                self.config['retriever_name'] = retriever\n",
    "                retrievers.append(self.init_retriever())\n",
    "                \n",
    "            self.retriever = EnsembleRetriever(retrievers=retrievers,\n",
    "                                              weights=self.config['ensemble_retrievers_weights'])\n",
    "            self.config['retriever_name'] = 'ensemble'\n",
    "            \n",
    "        elif self.config['retriever_name'] == 'BM25':\n",
    "            # Initialize the BM25 retriever with top_k relevance chunks\n",
    "            self.retriever = BM25Retriever.from_documents(documents=self.chunks)\n",
    "            self.retriever.k = self.config['retriever_k']\n",
    "\n",
    "        elif self.config['retriever_name'] == 'vectorstore':\n",
    "            # Initialize the vectorstore retriever\n",
    "            self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": self.config['retriever_k']})\n",
    "        else:\n",
    "            # Another retriever options aren't supported in this version :(\n",
    "            ValueError('Incorrect retriever name')\n",
    "        \n",
    "        return self.retriever\n",
    "    \n",
    "    \n",
    "    def init_compressor(self):\n",
    "        \"\"\"\n",
    "        Initialize the compressor.\n",
    "        If the compressor name is 'gluing_chunks', the compressor is initialized as a ChunkCompressor.\n",
    "        If the compressor name is 'cross_encoder_reranker', the compressor is initialized as a CrossEncoderReranker.\n",
    "        \"\"\"\n",
    "        if self.config['compressor_name'] == 'gluing_chunks':\n",
    "            # Initialize the common compressor class for gluing\n",
    "            self.compressor = ChunkCompressor(chunks=self.chunks, chunk_overlap=self.config['chunk_overlap'])\n",
    "        elif self.config['compressor_name'] == 'cross_encoder_reranker':\n",
    "            # Initialize the cross-encoder reranker\n",
    "            model = HuggingFaceCrossEncoder(model_name=self.config['reranker_model'])\n",
    "            self.compressor = CrossEncoderReranker(model=model, top_n=4)\n",
    "    \n",
    "    \n",
    "    def setup_qa_chain(self, custom_prompt: str = None):\n",
    "        \"\"\"\n",
    "        Set up the QA chain.\n",
    "        The retriever and compressor are initialized, and the QA chain is set up with the given custom prompt.\n",
    "\n",
    "        Parameters:\n",
    "        custom_prompt (str): The custom prompt for the QA chain.\n",
    "        \"\"\"\n",
    "        self.init_retriever()\n",
    "        self.init_compressor()\n",
    "        \n",
    "        \n",
    "        if self.compressor:\n",
    "            # Build a retriever-compressor chain if compressor exists             \n",
    "            compression_retriever = ContextualCompressionRetriever(\n",
    "                base_compressor=self.compressor, \n",
    "                base_retriever=self.retriever\n",
    "            )\n",
    "        else:\n",
    "            # Otherwise put only a retriever on a QA chain\n",
    "            compression_retriever = self.retriever\n",
    "                \n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=custom_prompt\n",
    "        )\n",
    "        # Build a QA chain with the given prompt and the class attributes\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=self.config['chain_type'],\n",
    "            retriever=compression_retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt_template}\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Query the pipeline with the given question.\n",
    "\n",
    "        Parameters:\n",
    "        question (str): The question to query the pipeline with.\n",
    "\n",
    "        Returns:\n",
    "        Dict: The result of the query.\n",
    "        \"\"\"\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain not set up. Call setup_qa_chain() first.\")\n",
    "        \n",
    "        # Run the QA chain with the provided question\n",
    "        return self.qa_chain({\"query\": question})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the pipeline                   \n",
    "    rag_pipeline = CustomRAGPipeline(documents_path=\"hmao_npa.txt\", config=config)\n",
    "    \n",
    "    # Load and process documents\n",
    "    rag_pipeline.load_and_process_documents()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a7dc78f-2228-4a9c-aad1-8b7e35c26c78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T22:56:11.246887Z",
     "iopub.status.busy": "2024-09-13T22:56:11.245870Z",
     "iopub.status.idle": "2024-09-13T22:56:14.176898Z",
     "shell.execute_reply": "2024-09-13T22:56:14.176092Z",
     "shell.execute_reply.started": "2024-09-13T22:56:11.246850Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it, est. speed input: 301.68 toks/s, output: 23.18 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Должностной оклад — фиксированный размер оплаты труда работника за исполнение трудовых (должностных) обязанностей определенной сложности за календарный месяц без учета компенсационных, стимулирующих и социальных выплат.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = '''Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just s\n",
    "ay that you don't know, don't try to make up an answer.\n",
    "Think step by step. Give full answer. Answer only in Russian. If context doesnt match the answer, say that you do not know the answer.\n",
    "{context}'''\n",
    "user_prompt = '''Question: {question}\n",
    "Answer:'''\n",
    "\n",
    "custom_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{user_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "rag_pipeline.setup_qa_chain(custom_prompt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # result = rag_pipeline.query(\"Какой герб изображен на бланках и штампах Комитета по средствам массовой информации и полиграфии Ханты-Мансийского автономного округа?\")\n",
    "    result = rag_pipeline.query(\"Что такое должностной оклад и как он рассчитывается?\")\n",
    "    # result = rag_pipeline.query(\"Какие мероприятия проводит Департамент охраны окружающей среды и экологической безопасности автономного округа в 2010 году?\")\n",
    "    # result = rag_pipeline.query('Когда юридические лица и ИП должны сообщать об аварийных выбросах?')\n",
    "    print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4d88e-0649-48e9-8074-1bd404f40a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
    "    \"\"\"\n",
    "    Create dataset for model evaluation with ragas library.\n",
    "\n",
    "    Parameters:\n",
    "    rag_pipeline (CustomRAGPipeline): Initialized instance of CustomRAGPipeline class\n",
    "    eval_dataset (pd.DataFrame): Prepared dataset for metrics calculation\n",
    "\n",
    "    Returns:\n",
    "    rag_eval_dataset(pd.DataFrame): The results with model answers and contexts.\n",
    "    \"\"\"\n",
    "    rag_dataset = []\n",
    "    for index, row in tqdm(eval_dataset.iterrows()):\n",
    "        answer = rag_pipeline.query(row[\"question\"])\n",
    "        rag_dataset.append(\n",
    "            {\"question\" : row[\"question\"],\n",
    "             \"answer\" : answer[\"result\"],\n",
    "             \"contexts\" : [context.page_content for context in answer[\"source_documents\"]],\n",
    "             \"ground_truth\" : row[\"ground_truth\"]\n",
    "             }\n",
    "        )\n",
    "    rag_df = pd.DataFrame(rag_dataset)\n",
    "    rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "    return rag_eval_dataset\n",
    "\n",
    "\n",
    "eval_dataset = pd.read_excel('v2_ragas_npa_dataset_firstPart.xlsx')\n",
    "eval_dataset = eval_dataset.groupby('evolution_type', group_keys=False).apply(lambda x: x.sample(25, random_state=42)).copy()\n",
    "eval_df = create_ragas_dataset(rag_pipeline, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742def37-38e2-4e2d-a1dd-3b5c9e4ecfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.save_to_disk('eval_df_baseline_new.hf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
