{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed4f8b-a87b-47ea-8fb4-d2cf094e558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from huggingface_hub import hf_hub_download\n",
    "from vllm import LLM, SamplingParams\n",
    "from langchain_community.llms import VLLM\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_utilization, context_recall, answer_correctness \n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f3a01-2fb3-486b-967b-b6de5e1cb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_name': 'llama3.1-8b-q4',  # llama3.1-8b-q4 / gemma-2-9b-it-simpo-q4 / tlite-q4\n",
    "    'embed_model_name_short': 'e5l', # e5l (multilingual-e5-large) /\n",
    "    'chunk_size': 512,\n",
    "    'chunk_overlap': 128,\n",
    "    'llm_framework': 'VLLM', # VLLM, LLamaCpp, Ollama\n",
    "    'vectorstore_name': 'MILVUS', # база данных MILVUS / FAISS\n",
    "    'retriever_type': None,\n",
    "    'reranker_type': None,\n",
    "    'chain_type': 'stuff',\n",
    "}\n",
    "\n",
    "llama_config = {\n",
    "    'repo_id': 'lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "    'filename': 'Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf',\n",
    "    'tokenizer': 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4'\n",
    "    }\n",
    "\n",
    "gemma_config = {\n",
    "    'repo_id': \"mannix/gemma2-9b-simpo\",\n",
    "    'llm_framework': 'Ollama'\n",
    "    }\n",
    "\n",
    "tlite_config = {\n",
    "    'repo_id': 'mradermacher/saiga_tlite_8b-GGUF',\n",
    "    'filename': 'saiga_tlite_8b.Q4_K_M.gguf',\n",
    "    'tokenizer': 'IlyaGusev/saiga_tlite_8b'\n",
    "    }\n",
    "\n",
    "def update_config_with_model(config, llama_config, gemma_config, tlite_config):\n",
    "    if config['model_name'] == 'llama3.1-8b-q4':\n",
    "        config.update(llama_config)\n",
    "    elif config['model_name'] == 'gemma-2-9b-it-simpo-q4':\n",
    "        config.update(gemma_config)\n",
    "    elif config['model_name'] == 'tlite-q4':\n",
    "        config.update(tlite_config)\n",
    "    else:\n",
    "        ValueError('Incorrect model_name: choose from llama3.1-8b-q4, gemma-2-9b-it-simpo-q4, or tlite-q4')\n",
    "    \n",
    "    if config['embed_model_name_short'] == 'e5l':\n",
    "        config['embedding_model'] = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "\n",
    "update_config_with_model(config, llama_config, gemma_config, tlite_config)\n",
    "\n",
    "class CustomRAGPipeline:\n",
    "    def __init__(self, \n",
    "                 documents_path: str,\n",
    "                 config: dict,\n",
    "                 recalc_embedding: bool = False,\n",
    "                 ):\n",
    "        \n",
    "        self.config = config\n",
    "        self.documents_path = documents_path\n",
    "        self.embedding_model = self.config['embedding_model']\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
    "\n",
    "        self.vectorstore_path = '_'.join([self.config['embed_model'], \n",
    "                                          self.config['vectorstore_name'], \n",
    "                                          str(self.config['chunk_size']), \n",
    "                                          str(self.config['chunk_overlap'])])\n",
    "\n",
    "        if not recalc_embedding:\n",
    "            if os.path.exists(self.vectorstore_path) and self.config['vectorstore_name'] == 'FAISS':\n",
    "                self.vectorstore = FAISS.load_local(self.vectorstore_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "            elif os.path.isfile(f\"{self.vectorstore_path}.db\") and self.config['vectorstore_name'] == 'MILVUS':\n",
    "                self.vectorstore = Milvus(\n",
    "                    self.embeddings,\n",
    "                    connection_args={\"uri\": f\"./{self.vectorstore_path}.db\"},\n",
    "                    collection_name=\"RAG\",\n",
    "                )\n",
    "\n",
    "        if self.config['llm_framework'] == 'VLLM':\n",
    "            self.llm = self.load_vllm_model()\n",
    "        elif self.config['llm_framework'] == 'LLamaCpp':\n",
    "            self.llm = self.load_llama_cpp_model()\n",
    "        elif self.config['llm_framework'] == 'Ollama':\n",
    "            self.llm = self.load_ollama_model()\n",
    "            \n",
    "            \n",
    "    def load_vllm_model(self):\n",
    "        # Load the vLLM model from HuggingFace Hub\n",
    "        repo_id = self.config['repo_id']\n",
    "        filename = self.config['filename']\n",
    "        tokenizer = self.config['tokenizer']\n",
    "        model_path = hf_hub_download(repo_id, filename=filename)\n",
    "        \n",
    "        # Initialize vLLM with the downloaded model\n",
    "        vllm_llm = VLLM(model=model_path,\n",
    "                        vllm_kwargs={\"quantization\": \"awq\", \n",
    "                                     'max_model_len': 13000,\n",
    "                                     'gpu_memory_utilization': 0.75},\n",
    "                        temperature=0.75,\n",
    "                        stop=[\"<|eot_id|>\"]\n",
    "                        )\n",
    "        \n",
    "        return vllm_llm\n",
    "\n",
    "\n",
    "    def load_llama_cpp_model(self):\n",
    "        repo_id = self.config['repo_id']\n",
    "        filename = self.config['filename']\n",
    "        model_path = hf_hub_download(repo_id, filename=filename)\n",
    "        \n",
    "        # Инициализация модели LlamaCpp\n",
    "        llama_cpp_llm = LlamaCpp(model_path=model_path,\n",
    "                                temperature=0.8,\n",
    "                                top_p=0.95,\n",
    "                                top_k=30,\n",
    "                                max_tokens=64,\n",
    "                                n_ctx=13000,\n",
    "                                n_parts=-1,\n",
    "                                n_gpu_layers=64,\n",
    "                                n_threads=8,\n",
    "                                frequency_penalty=1.1,\n",
    "                                verbose=True,\n",
    "                                stop=[\"<|eot_id|>\"],  # Остановка на токене EOS\n",
    "                                )\n",
    "        \n",
    "        return llama_cpp_llm\n",
    "\n",
    "    def load_ollama_model(self):\n",
    "        return OllamaLLM(model = self.config['repo_id'], \n",
    "                         temperature=0.8,\n",
    "                         top_p=0.95,\n",
    "                         top_k=30,\n",
    "                         max_tokens=512,\n",
    "                         stop=[\"<|eot_id|>\"])\n",
    "    \n",
    "    def load_and_process_documents(self):\n",
    "        if not self.vectorstore:\n",
    "            # Load documents from the specified path\n",
    "            loader = TextLoader(self.documents_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Split the documents into chunks\n",
    "            text_splitter = CharacterTextSplitter(\n",
    "                        separator=\" \",\n",
    "                        chunk_size=self.config['chunk_size'],\n",
    "                        chunk_overlap=self.config['chunk_overlap'],\n",
    "                        length_function=len,\n",
    "                        is_separator_regex=False,\n",
    "                    )\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            \n",
    "            if self.config['vectorstore_name'] == 'FAISS':\n",
    "                # Create a FAISS vector store from the documents\n",
    "                self.vectorstore = FAISS.from_documents(texts, self.embeddings)\n",
    "                self.vectorstore.save_local(self.vectorstore_path)\n",
    "            elif self.config['vectorstore_name'] == 'MILVUS':\n",
    "                Milvus.from_documents(\n",
    "                    texts,\n",
    "                    self.embeddings,\n",
    "                    collection_name=\"RAG\",\n",
    "                    connection_args={\"uri\": f\"./{self.vectorstore_path}.db\"})\n",
    "                \n",
    "    def setup_qa_chain(self, custom_prompt: str = None):\n",
    "        retriever = self.vectorstore.as_retriever()\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=custom_prompt\n",
    "        )\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=self.config['chain_type'],\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt_template}\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain not set up. Call setup_qa_chain() first.\")\n",
    "        \n",
    "        # Run the QA chain with the provided question\n",
    "        return self.qa_chain({\"query\": question})\n",
    "                    \n",
    "                    \n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the pipeline                   \n",
    "    rag_pipeline = CustomRAGPipeline(documents_path=\"hmao_npa.txt\", config=config)\n",
    "    \n",
    "    # Load and process documents\n",
    "    rag_pipeline.load_and_process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7dc78f-2228-4a9c-aad1-8b7e35c26c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Think step by step. Give full answer. Answer only in Russian. If context doesnt match the answer, say that you do not know the answer.\n",
    "{context}'''\n",
    "user_prompt = '''Question: {question}\n",
    "Answer:'''\n",
    "\n",
    "custom_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{user_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "rag_pipeline.setup_qa_chain(custom_prompt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # result = rag_pipeline.query(\"Какой герб изображен на бланках и штампах Комитета по средствам массовой информации и полиграфии Ханты-Мансийского автономного округа?\")\n",
    "    # print(result['result'])\n",
    "    # result = rag_pipeline.query(\"Что такое должностной оклад и как он рассчитывается?\")\n",
    "    # result = rag_pipeline.query(\"Какие мероприятия проводит Департамент охраны окружающей среды и экологической безопасности автономного округа в 2010 году?\")\n",
    "    result = rag_pipeline.query('Когда юридические лица и ИП должны сообщать об аварийных выбросах?')\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4d88e-0649-48e9-8074-1bd404f40a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для создания датасета для рагаса, потом удалить\n",
    "\n",
    "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
    "    rag_dataset = []\n",
    "    for index, row in tqdm(eval_dataset.iterrows()):\n",
    "        answer = rag_pipeline.query(row[\"question\"])\n",
    "        rag_dataset.append(\n",
    "            {\"question\" : row[\"question\"],\n",
    "             \"answer\" : answer[\"result\"],\n",
    "             \"contexts\" : [context.page_content for context in answer[\"source_documents\"]],\n",
    "             \"ground_truth\" : row[\"ground_truth\"]\n",
    "             }\n",
    "        )\n",
    "    rag_df = pd.DataFrame(rag_dataset)\n",
    "    rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "    return rag_eval_dataset\n",
    "\n",
    "eval_dataset = pd.read_excel('v2_ragas_npa_dataset_firstPart.xlsx')\n",
    "eval_dataset = eval_dataset.groupby('evolution_type', group_keys=False).apply(lambda x: x.sample(25, random_state=42)).copy()\n",
    "eval_df = create_ragas_dataset(rag_pipeline, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742def37-38e2-4e2d-a1dd-3b5c9e4ecfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.save_to_disk('eval_df_baseline_t_lite.hf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
