{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed4f8b-a87b-47ea-8fb4-d2cf094e558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from huggingface_hub import hf_hub_download\n",
    "from vllm import LLM, SamplingParams\n",
    "from langchain_community.llms import VLLM\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_utilization, context_recall, answer_correctness \n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f3a01-2fb3-486b-967b-b6de5e1cb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model': 'llama3.1-8b-quant4',\n",
    "          'repo_id': 'lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF',\n",
    "          'filename': 'Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf',\n",
    "          'tokenizer': 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4',\n",
    "          'embed_model': 'e5l', # краткое название, используется в именах\n",
    "          'embed_model_full': \"intfloat/multilingual-e5-large\", # huggingface embed model\n",
    "          'vectorstore_name': 'MILVUS', # база данных MILVUS / FAISS\n",
    "          'chunk_size': 512,\n",
    "          'chunk_overlap': 128,\n",
    "          'llm_framework': 'VLLM' # VLLM, LLamaCpp\n",
    "          'retriever': None,\n",
    "          'chain_type': 'stuff',\n",
    "}\n",
    "\n",
    "class CustomRAGPipeline:\n",
    "    def __init__(self, \n",
    "                 documents_path: str,\n",
    "                 config: dict,\n",
    "                 recalc_embedding: bool = False,\n",
    "                 ):\n",
    "        \n",
    "        self.config = config\n",
    "        self.documents_path = documents_path\n",
    "        self.embedding_model = self.config['embedding_model']\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
    "\n",
    "        self.vectorstore_path = '_'.join([self.config['embed_model'], \n",
    "                                          self.config['vectorstore_name'], \n",
    "                                          str(self.config['chunk_size']), \n",
    "                                          str(self.config['chunk_overlap'])])\n",
    "\n",
    "        if not recalc_embedding:\n",
    "            if os.path.exists(self.vectorstore_path) and self.config['vectorstore_name'] == 'FAISS':\n",
    "                self.vectorstore = FAISS.load_local(self.vectorstore_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "            elif recalc_embedding and os.path.isfile(f\"{self.vectorstore_path}.db\") and self.config['vectorstore_name'] == 'MILVUS':\n",
    "                self.vectorstore = Milvus(\n",
    "                    self.embeddings,\n",
    "                    connection_args={\"uri\": f\"./{self.vectorstore_path}.db\"},\n",
    "                    collection_name=\"RAG\",\n",
    "                )\n",
    "\n",
    "        if self.config['llm_framework'] == 'VLLM':\n",
    "            self.llm = self.load_vllm_model()\n",
    "        elif self.config['llm_framework'] == 'LLamaCpp':\n",
    "            self.llm = self.load_llama_cpp_model()\n",
    "            \n",
    "            \n",
    "    def load_vllm_model(self):\n",
    "        # Load the vLLM model from HuggingFace Hub\n",
    "        repo_id = self.config['repo_id']\n",
    "        filename = self.config['filename']\n",
    "        tokenizer = self.config['tokenizer']\n",
    "        model_path = hf_hub_download(repo_id, filename=filename)\n",
    "        \n",
    "        # Initialize vLLM with the downloaded model\n",
    "        vllm_llm = VLLM(model=model_path,\n",
    "                        vllm_kwargs={\"quantization\": \"awq\", \n",
    "                                     'max_model_len': 13000,\n",
    "                                     'gpu_memory_utilization': 0.75},\n",
    "                        temperature=0.75,\n",
    "                        )\n",
    "        \n",
    "        return vllm_llm\n",
    "\n",
    "\n",
    "    def load_llama_cpp_model(self):\n",
    "        repo_id = self.config['repo_id']\n",
    "        filename = self.config['filename']\n",
    "        model_path = hf_hub_download(repo_id, filename=filename)\n",
    "        \n",
    "        # Инициализация модели LlamaCpp\n",
    "        llama_cpp_llm = LlamaCpp(model_path=model_path,\n",
    "                                temperature=0.8,\n",
    "                                top_p=0.95,\n",
    "                                top_k=30,\n",
    "                                max_tokens=64,\n",
    "                                n_ctx=13000,\n",
    "                                n_parts=-1,\n",
    "                                n_gpu_layers=64,\n",
    "                                n_threads=8,\n",
    "                                frequency_penalty=1.1,\n",
    "                                verbose=True,\n",
    "                                stop=[\"<|eot_id|>\"],  # Остановка на токене EOS\n",
    "                                )\n",
    "        \n",
    "        return llama_cpp_llm\n",
    "\n",
    "    def load_and_process_documents(self):\n",
    "        if not self.vectorstore:\n",
    "            # Load documents from the specified path\n",
    "            loader = TextLoader(self.documents_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Split the documents into chunks\n",
    "            text_splitter = CharacterTextSplitter(\n",
    "                        separator=\" \",\n",
    "                        chunk_size=self.config['chunk_size'],\n",
    "                        chunk_overlap=self.config['chunk_overlap'],\n",
    "                        length_function=len,\n",
    "                        is_separator_regex=False,\n",
    "                    )\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            \n",
    "            if self.config['vectorstore_name'] == 'FAISS':\n",
    "                # Create a FAISS vector store from the documents\n",
    "                self.vectorstore = FAISS.from_documents(texts, self.embeddings)\n",
    "                self.vectorstore.save_local(self.vectorstore_path)\n",
    "            elif self.config['vectorstore_name'] == 'MILVUS':\n",
    "                Milvus.from_documents(\n",
    "                    texts,\n",
    "                    self.embeddings,\n",
    "                    collection_name=\"RAG\",\n",
    "                    connection_args={\"uri\": f\"./{self.vectorstore_path}.db\"})\n",
    "                \n",
    "    def setup_qa_chain(self, custom_prompt: str = None):\n",
    "        retriever = self.vectorstore.as_retriever()\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=custom_prompt\n",
    "        )\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=self.config['chain_type'],\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": prompt_template}\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain not set up. Call setup_qa_chain() first.\")\n",
    "        \n",
    "        # Run the QA chain with the provided question\n",
    "        return self.qa_chain({\"query\": question})\n",
    "                    \n",
    "                    \n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the pipeline                   \n",
    "    rag_pipeline = CustomRAGPipeline(documents_path=\"hmao_npa.txt\", config=config)\n",
    "    \n",
    "    # Load and process documents\n",
    "    rag_pipeline.load_and_process_documents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
