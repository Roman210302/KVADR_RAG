{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс RAG-пайплайна с выбранной конфигурацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14641/2613334185.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 18:11:03 config.py:1559] Downcasting torch.float32 to torch.float16.\n",
      "WARNING 09-05 18:11:03 config.py:318] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 09-05 18:11:03 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/home/danis/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', speculative_config=None, tokenizer='/home/danis/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.GGUF, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/danis/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 18:11:27 model_runner.py:879] Starting to load model /home/danis/.cache/huggingface/hub/models--lmstudio-community--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/8601e6db71269a2b12255ebdf09ab75becf22cc8/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf...\n",
      "INFO 09-05 18:11:42 model_runner.py:890] Loading model weights took 4.7372 GB\n",
      "INFO 09-05 18:11:54 gpu_executor.py:121] # GPU blocks: 578, # CPU blocks: 2048\n",
      "INFO 09-05 18:11:56 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-05 18:11:56 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-05 18:12:27 model_runner.py:1300] Graph capturing finished in 31 secs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from huggingface_hub import hf_hub_download\n",
    "from vllm import LLM, SamplingParams\n",
    "from langchain_community.llms import VLLM\n",
    "import faiss\n",
    "\n",
    "# Set up model configuration\n",
    "config = {'model': 'llama3.1-8b-quant4',\n",
    "          'embed_model': 'multilingual-e5-large',\n",
    "          'vectorstore_name': 'FAISS'}\n",
    "\n",
    "class CustomRAGPipeline:\n",
    "    def __init__(self, \n",
    "                 documents_path: str,\n",
    "                 config: dict,\n",
    "                 embedding_model: str = \"intfloat/multilingual-e5-large\",\n",
    "                 chunk_size: int = 1024,\n",
    "                 chunk_overlap: int = 256,\n",
    "                 recalc_embedding: bool = False,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Method for initialization a class instance\n",
    "\n",
    "        Args:\n",
    "            documents_path (str): path to text documents. \n",
    "            config (dict): pipeline configuration.\n",
    "            embedding_model (str, optional): model for generation text embeddings. Defaults to \"intfloat/multilingual-e5-large\".\n",
    "            chunk_size (int, optional): size of one chunk in tokens. Defaults to 1024.\n",
    "            chunk_overlap (int, optional): size of overlapping between neighboring chunks. Defaults to 256. \n",
    "            recalc_embedding (bool, optional): flag is it necessary to recalculate embeddings. Defaults to False.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.documents_path = documents_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.config = config\n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model)\n",
    "\n",
    "        self.vectorstore_path = self.config['embed_model'] + self.config['vectorstore_name']\n",
    "        if not recalc_embedding and os.path.exists(self.vectorstore_path):\n",
    "            # Load a local ready to use vector store if exists \n",
    "            self.vectorstore = new_vector_store = FAISS.load_local(self.vectorstore_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "            \n",
    "\n",
    "        self.llm = self.load_vllm_model()\n",
    "        \n",
    "        \n",
    "    def load_vllm_model(self):\n",
    "        '''\n",
    "        Method for load a language model from Hugging Face Hub with custom parameters\n",
    "        '''\n",
    "        # Load the vLLM model\n",
    "        repo_id = \"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "        filename = \"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
    "        model_path = hf_hub_download(repo_id, filename=filename)\n",
    "        \n",
    "        # Initialize a downloaded model\n",
    "        vllm_llm = VLLM(model=model_path,\n",
    "                        tokenizer='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4',\n",
    "                        vllm_kwargs={\"quantization\": \"awq\", \n",
    "                                     'max_model_len': 5000,\n",
    "                                     'gpu_memory_utilization': 0.65},\n",
    "                        temperature=0.2,\n",
    "                        )\n",
    "        \n",
    "        return vllm_llm\n",
    "\n",
    "    def load_and_process_documents(self):\n",
    "        \"\"\"\n",
    "        Method for processing text document for vector store\n",
    "        \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            # Load text documents from the specified path\n",
    "            loader = TextLoader(self.documents_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Split the using documents into chunks of \n",
    "            text_splitter = CharacterTextSplitter(\n",
    "                        separator=\" \",\n",
    "                        chunk_size=self.chunk_size\n",
    "                        chunk_overlap=self.chunk_overlap \n",
    "                        length_function=len,\n",
    "                        is_separator_regex=False,\n",
    "                    )\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Create a FAISS vector store from the documents\n",
    "            self.vectorstore = FAISS.from_documents(texts, self.embeddings)\n",
    "            self.vectorstore.save_local(self.vectorstore_path)\n",
    "        \n",
    "    def setup_qa_chain(self, custom_prompt: str = None):\n",
    "        \"\"\"\n",
    "        Method for setup a question-answering chain \n",
    "        Args:\n",
    "            custom_prompt (str, optional): transmitted prompt for qa chain. Defaults to None.\n",
    "        \"\"\"\n",
    "        retriever = self.vectorstore.as_retriever()\n",
    "        \n",
    "        if custom_prompt:\n",
    "            # Create a template of prompt\n",
    "            prompt_template = PromptTemplate(\n",
    "                input_variables=[\"context\", \"question\"],\n",
    "                template=custom_prompt\n",
    "            )\n",
    "            self.qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=retriever,\n",
    "                return_source_documents=True,\n",
    "                chain_type_kwargs={\"prompt\": prompt_template}\n",
    "            )\n",
    "        else:\n",
    "            self.qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=retriever,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "    \n",
    "    def query(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Method for generate a query for LLM\n",
    "\n",
    "        Args:\n",
    "            question (str): question of interest to the user\n",
    "\n",
    "        Raises:\n",
    "            ValueError: exception due to inability to receive prompt \n",
    "\n",
    "        Returns:\n",
    "            Dict: prompt of chosen template for LLM\n",
    "        \"\"\"\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"QA chain not set up. Call setup_qa_chain() first.\")\n",
    "        \n",
    "        # Run the QA chain with the provided question\n",
    "        return self.qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск пайплайна "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.42s/it, est. speed input: 105.20 toks/s, output: 38.15 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Департамент охраны окружающей среды и экологической безопасности автономного округа проводит следующие мероприятия в 20-10 году:\n",
      "     1) Конкурс - акция «Экология и мы» (март-июнь);\n",
      "     2) Окружной субботник «Наш чистый светлый дом - Югорская земля» (май-июнь);\n",
      "     3) Окружной конкурс «Лучшее муниципальное образование автономного округа в сфере охраны окружающей среды и природопользования» (май-декабрь);\n",
      "     4) Юбилейный Х слет школьных лесничеств и экологических объединений «Сохраним для обеспечения выживания и благополучных условий для размножения диких животных, боровой и водоплавающей дичи в критические периоды жизни» (сентябрь);\n",
      "     5) III Международная конференция ассоциированных школ ЮНЕСКО «Обь - Иртышский бассейн: молодежь изучает и сохраняет природное и культурное наследие в регионах великих рек мира» (май-июнь);\n",
      "     6) II Съезд экологов нефтяных регионов (в рамках VIII Международной экологической акции «Спасти и сохранить») (май);\n",
      "     7) V Всероссийская научно-практическая конференция «Экологическое образование и просвещение в интересах устойчивого развития» (в рамках VIII Международной экологической акции «Спасти и сохранить») (апрель);\n",
      "     8) Окружной молодёжный экологический форум «Сохраним цветущий мир Югры» (май-июнь);\n",
      "     9) Департамент охраны окружающей среды и экологической безопасности автономного округа также проводит научно-исследовательскую деятельность, эколого-просветительскую работу и обеспечивает содержание учреждений и выполнение возложенных на ООПТ задач.    .    .    .    .    .    .    .    .    .   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the RAG pipeline on hmao_npa.txt\n",
    "    rag_pipeline = CustomRAGPipeline(documents_path=\"hmao_npa.txt\", config=config)\n",
    "    \n",
    "    # Load and process documents from hmao_npa.txt\n",
    "    rag_pipeline.load_and_process_documents()\n",
    "\n",
    "    # If you want to use a custom prompt:\n",
    "    custom_prompt = \"\"\"\n",
    "    Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Think step by step.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    # Set up the QA chain with custom prompt if it exists\n",
    "    rag_pipeline.setup_qa_chain(custom_prompt)\n",
    "    \n",
    "    # Query with the custom prompt\n",
    "    result = rag_pipeline.query(\"Какие мероприятия проводит Департамент охраны окружающей среды и экологической безопасности автономного округа в 2010 году?\")\n",
    "    # Answer of RAG pipeline\n",
    "    print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
